{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark ML imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StringIndexer, CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "import tempfile\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('airline').getOrCreate()\n",
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Delay Time Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "Found 3 items\n",
      "-rw-r--r--   3 nhu2 nhu2 17953941457 2022-03-01 21:23 /user/nhu2/airline/Data_flights_clean.csv\n",
      "-rw-r--r--   3 nhu2 nhu2  2002936814 2022-03-02 17:54 /user/nhu2/airline/Data_logistics_clean.csv\n",
      "-rw-r--r--   3 nhu2 nhu2  2302318602 2022-03-08 14:11 /user/nhu2/airline/Data_regression_clean.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls  /user/nhu2/airline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into PySpark\n",
    "df = spark.read.csv(\"/user/nhu2/airline/Data_regression_clean.csv\", inferSchema=True, header=True)\n",
    "#df = spark.read.csv(\"gs://airline_bigdata/Data/regression_clean/part-00000-b781c44b-d818-4efc-99a2-80256e923831-c000.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OP_CARRIER', 'string'),\n",
       " ('ORIGIN', 'string'),\n",
       " ('CRS_DEP_TIME', 'double'),\n",
       " ('TAXI_OUT', 'double'),\n",
       " ('TAXI_IN', 'double'),\n",
       " ('CRS_ELAPSED_TIME', 'double'),\n",
       " ('SEASON', 'string'),\n",
       " ('label', 'double')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data type\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60431020, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dimension of dataset\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OP_CARRIER', 'string'),\n",
       " ('ORIGIN', 'string'),\n",
       " ('CRS_DEP_TIME', 'double'),\n",
       " ('TAXI_OUT', 'double'),\n",
       " ('TAXI_IN', 'double'),\n",
       " ('CRS_ELAPSED_TIME', 'double'),\n",
       " ('SEASON', 'string'),\n",
       " ('label', 'double')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rename label column to avoid illegal argument error\n",
    "\n",
    "#df = df.withColumnRenamed(\"label\",\"AVG_DELAY\")\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = [\"OP_CARRIER\",\n",
    "                     \"ORIGIN\",\n",
    "                     \"SEASON\"]\n",
    "\n",
    "numericalColumns = [\"CRS_DEP_TIME\",\n",
    "                    \"TAXI_OUT\",\n",
    "                   \"TAXI_IN\",\n",
    "                   \"CRS_ELAPSED_TIME\"]\n",
    "\n",
    "categoricalColumnsclassVec = [c + \"classVec\" for c in categoricalColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_CARRIER\n",
      "ORIGIN\n",
      "SEASON\n"
     ]
    }
   ],
   "source": [
    "stages = []\n",
    "for categoricalColumn in categoricalColumns:\n",
    "    print(categoricalColumn)\n",
    "  ## Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalColumn, outputCol = categoricalColumn+\"Index\").setHandleInvalid(\"skip\")\n",
    "  ## Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCol=categoricalColumn+\"Index\", outputCol=categoricalColumn+\"classVec\")\n",
    "  ## Add stages\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "catassembler = VectorAssembler(inputCols = categoricalColumnsclassVec,\n",
    "                            outputCol = \"catfeatures\")\n",
    "stages += [catassembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only scale the numeric columns \n",
    "numassembler = VectorAssembler(inputCols = numericalColumns,\n",
    "                            outputCol = \"numfeatures\")\n",
    "stages += [numassembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol = \"numfeatures\",\n",
    "                        outputCol = \"scaledFeatures\",\n",
    "                        withStd = True,\n",
    "                        withMean = True)\n",
    "stages += [scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both \n",
    "totalassembler = VectorAssembler(inputCols = ['catfeatures','scaledFeatures'],\n",
    "                            outputCol = \"features\")\n",
    "stages += [totalassembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Suspect that the error comes from transforming separately?\n",
    "# Yep \n",
    "# Although this introduces data leakage, it seems like splitting before transforming will cause illegal argument error\n",
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.select(['features', 'label']).randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                      |label|\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(406,[0,25,399,402,403,404,405],[1.0,1.0,1.0,-1.5301571430162986,-0.8586483440519849,-0.3997735200289499,0.31077443596073556])|-8.0 |\n",
      "|(406,[0,25,399,402,403,404,405],[1.0,1.0,1.0,-1.5301571430162986,-0.7511117112189889,-0.5888811502527409,-0.7098328064051183])|-8.0 |\n",
      "|(406,[0,25,399,402,403,404,405],[1.0,1.0,1.0,-1.5301571430162986,-0.7511117112189889,-0.5888811502527409,-0.7098328064051183])|-4.0 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_df.show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(406,[0,25,399,40...| -7.5|\n",
      "|(406,[0,25,399,40...| -7.5|\n",
      "|(406,[0,25,399,40...| -7.0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='label')\n",
    "m1 = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 36.6294\n"
     ]
    }
   ],
   "source": [
    "reg_pred = m1.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 36.585\n",
      "MAE: 17.941\n"
     ]
    }
   ],
   "source": [
    "evalu = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "\n",
    "rmse = evalu.evaluate(reg_pred, {evalu.metricName: \"rmse\"})\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "mae = evalu.evaluate(reg_pred, {evalu.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized Linear Regression with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='label')\n",
    "lrevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "#Define parameter grid\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.maxIter, [200, 500, 1000])\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 1, 10, 20])\n",
    "             .addGrid(lr.elasticNetParam, [0, 0.1, 0.25, 0.5, 0.75, 1])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv = CrossValidator(estimator = lr,\n",
    "                    estimatorParamMaps = lrparamGrid,\n",
    "                    evaluator = lrevaluator,\n",
    "                    numFolds = 3,\n",
    "                    parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_reg = lrcv.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_reg_pred = cv_reg.bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_reg.bestModel._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_reg.bestModel._java_obj.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_reg.bestModel._java_obj.getElasticNetParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 36.585\n",
      "MAE: 17.949\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Square Error\n",
    "rmse = evalu.evaluate(cv_reg_pred, {evalu.metricName: \"rmse\"})\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = evalu.evaluate(cv_reg_pred, {evalu.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Load data into PySpark\n",
    "df = spark.read.csv(\"gs://dataproc-staging-us-central1-1082458152958-9ewhbauy/Cleaned_Data/Data_regression_clean_part-00000-b781c44b-d818-4efc-99a2-80256e923831-c000.csv\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 36.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 18.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evalu = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "# Root Mean Square Error\n",
    "rmse = evalu.evaluate(pred_rf, {evalu.metricName: \"rmse\"})\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = evalu.evaluate(pred_rf, {evalu.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
